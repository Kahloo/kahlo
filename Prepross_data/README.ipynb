{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "<img src=\"logo.jpg\", width=150, ALIGN=\"left\", border=20>\n",
    "<center>\n",
    "<h1>Sample Starting Kit </h1>\n",
    " <br>This code was tested with <br>\n",
    "Python 3.6.6 |Anaconda custom (64-bit)| (default, Jun 28 2018, 11:07:29) (https://anaconda.org/)<br>\n",
    "<i> Adapted for Chalab by Isabelle Guyon from original code of Balázs Kégl</i> <br>\n",
    "<a href=\"http://www.datascience-paris-saclay.fr\">Paris Saclay Center for Data Science (CDS)</a>\n",
    "</center>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<p>\n",
    "ALL INFORMATION, SOFTWARE, DOCUMENTATION, AND DATA ARE PROVIDED \"AS-IS\". The CDS, CHALEARN, AND/OR OTHER ORGANIZERS OR CODE AUTHORS DISCLAIM ANY EXPRESSED OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR ANY PARTICULAR PURPOSE, AND THE WARRANTY OF NON-INFRIGEMENT OF ANY THIRD PARTY'S INTELLECTUAL PROPERTY RIGHTS. IN NO EVENT SHALL AUTHORS AND ORGANIZERS BE LIABLE FOR ANY SPECIAL, \n",
    "INDIRECT OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES WHATSOEVER ARISING OUT OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF SOFTWARE, DOCUMENTS, MATERIALS, PUBLICATIONS, OR INFORMATION MADE AVAILABLE FOR THE CHALLENGE. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Introduction </h2>\n",
    "    <p>\n",
    "In an era where computer graphics techniques for image generation are reaching stunning levels of quality, it becomes more and more challenging to detect fake from true, authentic images. However, this raises a lot of legal issues, mainly dealing with forgery. This project focuses mainly on the efficiency of Generative Adversarial Network (GAN) algorithms for producing art forgery and we will try to beat the state-of-the-art models that detect it. In fact, the Fine Arts Expert Institute in Geneva estimates that as much as 50 percent of artworks currently in circulation may be forgeries. Although the standard approach for image classification which is deep neural networks and especially Convolutional Neural Networks (CNNs) works very well nowadays, GAN performs strongly on generating fake images. By focusing on the art forgery aspect, we will try to bring a more specific perspective on the issue of image forgery.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Requirements\n",
    "Our code uses multiple libraries, so the next cell will install python's required dependencies. In case you don't want to, or are running in the competition's docker, you can comment it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install --user -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "model_dir = 'sample_code_submission/'                        # Change the model to a better one once you have one!\n",
    "result_dir = 'sample_result_submission/' \n",
    "problem_dir = 'ingestion_program/'  \n",
    "score_dir = 'scoring_program/'\n",
    "from sys import path; path.append(model_dir); path.append(problem_dir); path.append(score_dir); \n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import seaborn as sns; sns.set()\n",
    "import warnings\n",
    "import os\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h1> Step 1: Exploratory data analysis </h1>\n",
    "<p>\n",
    "We provide sample_data with the starting kit, but to prepare your submission, you must fetch the public_data from the challenge website and point to it.\n",
    "    <br>\n",
    "    <span style=\"color:red\"> Just change the data name in the block below. In the rest of the section, replace the sample plots by anything you want. </span>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' n’est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exécutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "data_dir = 'public_data'         \n",
    "data_name = 'perso'\n",
    "!ls $data_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading public_data/perso_train from AutoML format\n",
      "Number of examples = 65856\n",
      "Number of features = 200\n",
      "   Class\n",
      "0  False\n",
      "1   True\n",
      "Number of classes = 2\n"
     ]
    }
   ],
   "source": [
    "from data_io import read_as_df\n",
    "data = read_as_df(data_dir  + '/' + data_name)          # The perso_data is loaded as a Pandas Data Frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature0</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature191</th>\n",
       "      <th>feature192</th>\n",
       "      <th>feature193</th>\n",
       "      <th>feature194</th>\n",
       "      <th>feature195</th>\n",
       "      <th>feature196</th>\n",
       "      <th>feature197</th>\n",
       "      <th>feature198</th>\n",
       "      <th>feature199</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-720.579708</td>\n",
       "      <td>-330.571966</td>\n",
       "      <td>-2188.381016</td>\n",
       "      <td>1636.259793</td>\n",
       "      <td>450.914411</td>\n",
       "      <td>2896.124291</td>\n",
       "      <td>990.411094</td>\n",
       "      <td>-221.979955</td>\n",
       "      <td>735.522150</td>\n",
       "      <td>658.305425</td>\n",
       "      <td>...</td>\n",
       "      <td>48.580091</td>\n",
       "      <td>72.751124</td>\n",
       "      <td>140.823452</td>\n",
       "      <td>233.252833</td>\n",
       "      <td>-125.343139</td>\n",
       "      <td>-11.203929</td>\n",
       "      <td>-298.022503</td>\n",
       "      <td>268.679125</td>\n",
       "      <td>-107.863398</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-5322.933980</td>\n",
       "      <td>-2089.062676</td>\n",
       "      <td>-380.988992</td>\n",
       "      <td>1346.675402</td>\n",
       "      <td>-175.516347</td>\n",
       "      <td>-187.304026</td>\n",
       "      <td>-516.713746</td>\n",
       "      <td>299.742812</td>\n",
       "      <td>737.662192</td>\n",
       "      <td>367.725877</td>\n",
       "      <td>...</td>\n",
       "      <td>-11.319036</td>\n",
       "      <td>51.214428</td>\n",
       "      <td>-28.411065</td>\n",
       "      <td>-61.522861</td>\n",
       "      <td>3.005384</td>\n",
       "      <td>-0.986070</td>\n",
       "      <td>23.010128</td>\n",
       "      <td>26.574740</td>\n",
       "      <td>169.194344</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-4200.092388</td>\n",
       "      <td>-1871.126468</td>\n",
       "      <td>447.135529</td>\n",
       "      <td>-1006.329809</td>\n",
       "      <td>-462.005764</td>\n",
       "      <td>74.704184</td>\n",
       "      <td>-389.507083</td>\n",
       "      <td>1048.262956</td>\n",
       "      <td>-943.557088</td>\n",
       "      <td>-372.852894</td>\n",
       "      <td>...</td>\n",
       "      <td>-59.611552</td>\n",
       "      <td>25.703768</td>\n",
       "      <td>-294.928181</td>\n",
       "      <td>35.586727</td>\n",
       "      <td>-82.896835</td>\n",
       "      <td>-139.520672</td>\n",
       "      <td>67.240814</td>\n",
       "      <td>130.350498</td>\n",
       "      <td>11.225183</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-3185.549644</td>\n",
       "      <td>306.772873</td>\n",
       "      <td>433.077977</td>\n",
       "      <td>-210.809758</td>\n",
       "      <td>36.321483</td>\n",
       "      <td>-219.512936</td>\n",
       "      <td>-1054.496076</td>\n",
       "      <td>-410.837075</td>\n",
       "      <td>-1310.575500</td>\n",
       "      <td>-722.056874</td>\n",
       "      <td>...</td>\n",
       "      <td>-81.840073</td>\n",
       "      <td>70.061337</td>\n",
       "      <td>-167.365191</td>\n",
       "      <td>-33.844216</td>\n",
       "      <td>101.946416</td>\n",
       "      <td>-31.338004</td>\n",
       "      <td>-113.008504</td>\n",
       "      <td>19.829202</td>\n",
       "      <td>20.704420</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4225.769673</td>\n",
       "      <td>2321.933013</td>\n",
       "      <td>340.961107</td>\n",
       "      <td>1458.868017</td>\n",
       "      <td>-149.674140</td>\n",
       "      <td>374.538303</td>\n",
       "      <td>1012.925872</td>\n",
       "      <td>-1814.882862</td>\n",
       "      <td>161.417185</td>\n",
       "      <td>-526.341569</td>\n",
       "      <td>...</td>\n",
       "      <td>65.427990</td>\n",
       "      <td>-65.920148</td>\n",
       "      <td>80.513158</td>\n",
       "      <td>-96.773541</td>\n",
       "      <td>52.067824</td>\n",
       "      <td>-55.347607</td>\n",
       "      <td>-50.855075</td>\n",
       "      <td>-38.701155</td>\n",
       "      <td>261.381070</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 201 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      feature0     feature1     feature2     feature3    feature4  \\\n",
       "0  -720.579708  -330.571966 -2188.381016  1636.259793  450.914411   \n",
       "1 -5322.933980 -2089.062676  -380.988992  1346.675402 -175.516347   \n",
       "2 -4200.092388 -1871.126468   447.135529 -1006.329809 -462.005764   \n",
       "3 -3185.549644   306.772873   433.077977  -210.809758   36.321483   \n",
       "4  4225.769673  2321.933013   340.961107  1458.868017 -149.674140   \n",
       "\n",
       "      feature5     feature6     feature7     feature8    feature9  ...  \\\n",
       "0  2896.124291   990.411094  -221.979955   735.522150  658.305425  ...   \n",
       "1  -187.304026  -516.713746   299.742812   737.662192  367.725877  ...   \n",
       "2    74.704184  -389.507083  1048.262956  -943.557088 -372.852894  ...   \n",
       "3  -219.512936 -1054.496076  -410.837075 -1310.575500 -722.056874  ...   \n",
       "4   374.538303  1012.925872 -1814.882862   161.417185 -526.341569  ...   \n",
       "\n",
       "   feature191  feature192  feature193  feature194  feature195  feature196  \\\n",
       "0   48.580091   72.751124  140.823452  233.252833 -125.343139  -11.203929   \n",
       "1  -11.319036   51.214428  -28.411065  -61.522861    3.005384   -0.986070   \n",
       "2  -59.611552   25.703768 -294.928181   35.586727  -82.896835 -139.520672   \n",
       "3  -81.840073   70.061337 -167.365191  -33.844216  101.946416  -31.338004   \n",
       "4   65.427990  -65.920148   80.513158  -96.773541   52.067824  -55.347607   \n",
       "\n",
       "   feature197  feature198  feature199  target  \n",
       "0 -298.022503  268.679125 -107.863398   False  \n",
       "1   23.010128   26.574740  169.194344   False  \n",
       "2   67.240814  130.350498   11.225183   False  \n",
       "3 -113.008504   19.829202   20.704420   False  \n",
       "4  -50.855075  -38.701155  261.381070   False  \n",
       "\n",
       "[5 rows x 201 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For convenience, we load the data as a \"pandas\" data frame, so we can use \"pandas\" and \"seaborn\" built in functions to explore the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature0</th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>feature5</th>\n",
       "      <th>feature6</th>\n",
       "      <th>feature7</th>\n",
       "      <th>feature8</th>\n",
       "      <th>feature9</th>\n",
       "      <th>...</th>\n",
       "      <th>feature190</th>\n",
       "      <th>feature191</th>\n",
       "      <th>feature192</th>\n",
       "      <th>feature193</th>\n",
       "      <th>feature194</th>\n",
       "      <th>feature195</th>\n",
       "      <th>feature196</th>\n",
       "      <th>feature197</th>\n",
       "      <th>feature198</th>\n",
       "      <th>feature199</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "      <td>65856.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>7.948228</td>\n",
       "      <td>4.639587</td>\n",
       "      <td>0.060584</td>\n",
       "      <td>0.855769</td>\n",
       "      <td>0.357277</td>\n",
       "      <td>7.274473</td>\n",
       "      <td>-1.869317</td>\n",
       "      <td>-1.103966</td>\n",
       "      <td>-2.037879</td>\n",
       "      <td>-1.444957</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.014606</td>\n",
       "      <td>-0.272984</td>\n",
       "      <td>-0.059105</td>\n",
       "      <td>0.246937</td>\n",
       "      <td>0.403709</td>\n",
       "      <td>0.143219</td>\n",
       "      <td>0.073579</td>\n",
       "      <td>-0.077973</td>\n",
       "      <td>-0.060606</td>\n",
       "      <td>-0.243602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>4527.520299</td>\n",
       "      <td>2004.792136</td>\n",
       "      <td>1779.131964</td>\n",
       "      <td>1479.263773</td>\n",
       "      <td>1223.426107</td>\n",
       "      <td>1146.015959</td>\n",
       "      <td>969.066518</td>\n",
       "      <td>895.355556</td>\n",
       "      <td>786.722509</td>\n",
       "      <td>711.069693</td>\n",
       "      <td>...</td>\n",
       "      <td>125.708011</td>\n",
       "      <td>124.573000</td>\n",
       "      <td>124.085856</td>\n",
       "      <td>123.794939</td>\n",
       "      <td>122.868808</td>\n",
       "      <td>121.976787</td>\n",
       "      <td>122.081693</td>\n",
       "      <td>121.612663</td>\n",
       "      <td>120.540781</td>\n",
       "      <td>119.955238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-12742.430972</td>\n",
       "      <td>-9023.211696</td>\n",
       "      <td>-8421.497788</td>\n",
       "      <td>-9367.852309</td>\n",
       "      <td>-7187.540320</td>\n",
       "      <td>-6688.845174</td>\n",
       "      <td>-5695.292683</td>\n",
       "      <td>-5531.296531</td>\n",
       "      <td>-5411.785285</td>\n",
       "      <td>-4643.432207</td>\n",
       "      <td>...</td>\n",
       "      <td>-773.122053</td>\n",
       "      <td>-1027.098089</td>\n",
       "      <td>-1032.805878</td>\n",
       "      <td>-809.367433</td>\n",
       "      <td>-994.457441</td>\n",
       "      <td>-772.500641</td>\n",
       "      <td>-907.994302</td>\n",
       "      <td>-733.719626</td>\n",
       "      <td>-710.527240</td>\n",
       "      <td>-821.714025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-3274.891219</td>\n",
       "      <td>-1177.313799</td>\n",
       "      <td>-1102.805327</td>\n",
       "      <td>-807.709460</td>\n",
       "      <td>-680.398415</td>\n",
       "      <td>-682.350395</td>\n",
       "      <td>-585.396431</td>\n",
       "      <td>-534.121973</td>\n",
       "      <td>-434.339013</td>\n",
       "      <td>-412.187668</td>\n",
       "      <td>...</td>\n",
       "      <td>-73.133615</td>\n",
       "      <td>-73.276018</td>\n",
       "      <td>-73.564071</td>\n",
       "      <td>-73.897173</td>\n",
       "      <td>-73.313577</td>\n",
       "      <td>-71.965030</td>\n",
       "      <td>-72.125558</td>\n",
       "      <td>-72.216912</td>\n",
       "      <td>-70.905623</td>\n",
       "      <td>-71.805017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-195.286039</td>\n",
       "      <td>129.786316</td>\n",
       "      <td>-103.967754</td>\n",
       "      <td>146.154493</td>\n",
       "      <td>-3.505489</td>\n",
       "      <td>-18.044559</td>\n",
       "      <td>-55.113862</td>\n",
       "      <td>16.804429</td>\n",
       "      <td>-7.350562</td>\n",
       "      <td>6.081740</td>\n",
       "      <td>...</td>\n",
       "      <td>1.444448</td>\n",
       "      <td>1.216627</td>\n",
       "      <td>-0.868476</td>\n",
       "      <td>-0.641931</td>\n",
       "      <td>-0.213107</td>\n",
       "      <td>0.625010</td>\n",
       "      <td>-1.533394</td>\n",
       "      <td>-0.130634</td>\n",
       "      <td>0.410809</td>\n",
       "      <td>0.289759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3083.881246</td>\n",
       "      <td>1257.955175</td>\n",
       "      <td>952.530743</td>\n",
       "      <td>952.914092</td>\n",
       "      <td>671.960261</td>\n",
       "      <td>682.672999</td>\n",
       "      <td>530.041801</td>\n",
       "      <td>541.486421</td>\n",
       "      <td>428.102311</td>\n",
       "      <td>411.366246</td>\n",
       "      <td>...</td>\n",
       "      <td>73.798601</td>\n",
       "      <td>72.928751</td>\n",
       "      <td>72.739949</td>\n",
       "      <td>73.366181</td>\n",
       "      <td>73.679866</td>\n",
       "      <td>72.218836</td>\n",
       "      <td>71.430684</td>\n",
       "      <td>72.728981</td>\n",
       "      <td>71.713926</td>\n",
       "      <td>71.651149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15055.474982</td>\n",
       "      <td>10698.120915</td>\n",
       "      <td>9554.132261</td>\n",
       "      <td>11444.055706</td>\n",
       "      <td>8174.589084</td>\n",
       "      <td>6336.943285</td>\n",
       "      <td>7427.424157</td>\n",
       "      <td>5547.592429</td>\n",
       "      <td>6218.223714</td>\n",
       "      <td>5376.299435</td>\n",
       "      <td>...</td>\n",
       "      <td>796.276702</td>\n",
       "      <td>1421.298186</td>\n",
       "      <td>744.646970</td>\n",
       "      <td>901.421396</td>\n",
       "      <td>1017.062212</td>\n",
       "      <td>882.567186</td>\n",
       "      <td>1289.488162</td>\n",
       "      <td>809.657357</td>\n",
       "      <td>833.619205</td>\n",
       "      <td>827.775638</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 200 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           feature0      feature1      feature2      feature3      feature4  \\\n",
       "count  65856.000000  65856.000000  65856.000000  65856.000000  65856.000000   \n",
       "mean       7.948228      4.639587      0.060584      0.855769      0.357277   \n",
       "std     4527.520299   2004.792136   1779.131964   1479.263773   1223.426107   \n",
       "min   -12742.430972  -9023.211696  -8421.497788  -9367.852309  -7187.540320   \n",
       "25%    -3274.891219  -1177.313799  -1102.805327   -807.709460   -680.398415   \n",
       "50%     -195.286039    129.786316   -103.967754    146.154493     -3.505489   \n",
       "75%     3083.881246   1257.955175    952.530743    952.914092    671.960261   \n",
       "max    15055.474982  10698.120915   9554.132261  11444.055706   8174.589084   \n",
       "\n",
       "           feature5      feature6      feature7      feature8      feature9  \\\n",
       "count  65856.000000  65856.000000  65856.000000  65856.000000  65856.000000   \n",
       "mean       7.274473     -1.869317     -1.103966     -2.037879     -1.444957   \n",
       "std     1146.015959    969.066518    895.355556    786.722509    711.069693   \n",
       "min    -6688.845174  -5695.292683  -5531.296531  -5411.785285  -4643.432207   \n",
       "25%     -682.350395   -585.396431   -534.121973   -434.339013   -412.187668   \n",
       "50%      -18.044559    -55.113862     16.804429     -7.350562      6.081740   \n",
       "75%      682.672999    530.041801    541.486421    428.102311    411.366246   \n",
       "max     6336.943285   7427.424157   5547.592429   6218.223714   5376.299435   \n",
       "\n",
       "       ...    feature190    feature191    feature192    feature193  \\\n",
       "count  ...  65856.000000  65856.000000  65856.000000  65856.000000   \n",
       "mean   ...     -0.014606     -0.272984     -0.059105      0.246937   \n",
       "std    ...    125.708011    124.573000    124.085856    123.794939   \n",
       "min    ...   -773.122053  -1027.098089  -1032.805878   -809.367433   \n",
       "25%    ...    -73.133615    -73.276018    -73.564071    -73.897173   \n",
       "50%    ...      1.444448      1.216627     -0.868476     -0.641931   \n",
       "75%    ...     73.798601     72.928751     72.739949     73.366181   \n",
       "max    ...    796.276702   1421.298186    744.646970    901.421396   \n",
       "\n",
       "         feature194    feature195    feature196    feature197    feature198  \\\n",
       "count  65856.000000  65856.000000  65856.000000  65856.000000  65856.000000   \n",
       "mean       0.403709      0.143219      0.073579     -0.077973     -0.060606   \n",
       "std      122.868808    121.976787    122.081693    121.612663    120.540781   \n",
       "min     -994.457441   -772.500641   -907.994302   -733.719626   -710.527240   \n",
       "25%      -73.313577    -71.965030    -72.125558    -72.216912    -70.905623   \n",
       "50%       -0.213107      0.625010     -1.533394     -0.130634      0.410809   \n",
       "75%       73.679866     72.218836     71.430684     72.728981     71.713926   \n",
       "max     1017.062212    882.567186   1289.488162    809.657357    833.619205   \n",
       "\n",
       "         feature199  \n",
       "count  65856.000000  \n",
       "mean      -0.243602  \n",
       "std      119.955238  \n",
       "min     -821.714025  \n",
       "25%      -71.805017  \n",
       "50%        0.289759  \n",
       "75%       71.651149  \n",
       "max      827.775638  \n",
       "\n",
       "[8 rows x 200 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "<h1>Step 2: Building a predictive model</h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Loading data with DataManager</h2>\n",
    "    <p>\n",
    "We reload the data with the AutoML DataManager class because this is more convenient:\n",
    "  \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Info file found : C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\public_data\\perso_public.info\n",
      "DataManager : perso\n",
      "info:\n",
      "\tusage = Sample dataset perso data\n",
      "\tname = perso\n",
      "\ttask = bi-class.classification\n",
      "\ttarget_type = Numerical\n",
      "\tfeat_type = Numerical\n",
      "\tmetric = bac_metric\n",
      "\ttime_budget = 1200\n",
      "\tfeat_num = 200\n",
      "\ttarget_num = 2\n",
      "\tlabel_num = 2\n",
      "\ttrain_num = 65856\n",
      "\tvalid_num = 9408\n",
      "\ttest_num = 18817\n",
      "\thas_categorical = 0\n",
      "\thas_missing = 0\n",
      "\tis_sparse = 0\n",
      "\tformat = dense\n",
      "data:\n",
      "\tX_train = array(65856, 200)\n",
      "\tY_train = array(65856,)\n",
      "\tX_valid = array(9408, 200)\n",
      "\tY_valid = array(0,)\n",
      "\tX_test = array(18817, 200)\n",
      "\tY_test = array(0,)\n",
      "feat_type:\tarray(200,)\n",
      "feat_idx:\tarray(0,)\n",
      "\n",
      "perso\n"
     ]
    }
   ],
   "source": [
    "from data_manager import DataManager\n",
    "D = DataManager(data_name, data_dir, replace_missing=True)\n",
    "print(D)\n",
    "print(data_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Training a predictive model</h2>\n",
    "    <p>\n",
    "We provide an example of predictive model (for classification or regression) in the `sample_code_submission/` directory. It is a quite stupid model: it makes constant predictions. Replace it with your own model.\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy.core.umath_tests import inner1d\n",
    "from data_io import write\n",
    "from model import model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "an instance of the model (run the constructor) and attempt to reload a previously saved version from `sample_code_submission/`:\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perso\n",
      "sample_code_submission/\n",
      "sample_code_submission/perso\n"
     ]
    }
   ],
   "source": [
    "M = model()\n",
    "trained_model_name = model_dir + data_name\n",
    "print(data_name)\n",
    "print(model_dir)\n",
    "print(trained_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 504,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(65856, 200)\n"
     ]
    }
   ],
   "source": [
    "print(D.data['X_train'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    Train the model (unless you reloaded a trained model) and make predictions. \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 505,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 52684 samples, validate on 13172 samples\n",
      "Epoch 1/30\n",
      "52684/52684 [==============================] - 23s 440us/step - loss: 0.5010 - acc: 0.7548 - val_loss: 0.4351 - val_acc: 0.8009\n",
      "Epoch 2/30\n",
      "52684/52684 [==============================] - 15s 294us/step - loss: 0.3634 - acc: 0.8405 - val_loss: 0.3687 - val_acc: 0.8383\n",
      "Epoch 3/30\n",
      "52684/52684 [==============================] - 15s 293us/step - loss: 0.2842 - acc: 0.8811 - val_loss: 0.3431 - val_acc: 0.8549\n",
      "Epoch 4/30\n",
      "52684/52684 [==============================] - 16s 295us/step - loss: 0.2322 - acc: 0.9047 - val_loss: 0.3217 - val_acc: 0.8665\n",
      "Epoch 5/30\n",
      "52684/52684 [==============================] - 15s 294us/step - loss: 0.1933 - acc: 0.9228 - val_loss: 0.3441 - val_acc: 0.8671\n",
      "Epoch 6/30\n",
      "52684/52684 [==============================] - 16s 295us/step - loss: 0.1641 - acc: 0.9347 - val_loss: 0.3362 - val_acc: 0.8701\n",
      "Epoch 7/30\n",
      "52684/52684 [==============================] - 16s 297us/step - loss: 0.1417 - acc: 0.9448 - val_loss: 0.3339 - val_acc: 0.8742\n",
      "Epoch 8/30\n",
      "52684/52684 [==============================] - 16s 298us/step - loss: 0.1203 - acc: 0.9534 - val_loss: 0.3631 - val_acc: 0.8732\n",
      "Epoch 9/30\n",
      "52684/52684 [==============================] - 16s 295us/step - loss: 0.1047 - acc: 0.9592 - val_loss: 0.3667 - val_acc: 0.8744\n",
      "Epoch 10/30\n",
      "52684/52684 [==============================] - 16s 295us/step - loss: 0.0922 - acc: 0.9644 - val_loss: 0.4597 - val_acc: 0.8738\n",
      "Epoch 11/30\n",
      "52684/52684 [==============================] - 16s 297us/step - loss: 0.0841 - acc: 0.9677 - val_loss: 0.4381 - val_acc: 0.8780\n",
      "Epoch 12/30\n",
      "52684/52684 [==============================] - 16s 297us/step - loss: 0.0727 - acc: 0.9720 - val_loss: 0.4250 - val_acc: 0.8774\n",
      "Epoch 13/30\n",
      "52684/52684 [==============================] - 16s 300us/step - loss: 0.0662 - acc: 0.9751 - val_loss: 0.4884 - val_acc: 0.8772\n",
      "Epoch 14/30\n",
      "52684/52684 [==============================] - 16s 298us/step - loss: 0.0599 - acc: 0.9775 - val_loss: 0.5351 - val_acc: 0.8763\n",
      "Epoch 15/30\n",
      "52684/52684 [==============================] - 16s 300us/step - loss: 0.0571 - acc: 0.9788 - val_loss: 0.4571 - val_acc: 0.8782\n",
      "Epoch 16/30\n",
      "52684/52684 [==============================] - 16s 300us/step - loss: 0.0537 - acc: 0.9795 - val_loss: 0.5218 - val_acc: 0.8778\n",
      "Epoch 17/30\n",
      "52684/52684 [==============================] - 16s 299us/step - loss: 0.0496 - acc: 0.9823 - val_loss: 0.5661 - val_acc: 0.8782\n",
      "Epoch 18/30\n",
      "52684/52684 [==============================] - 16s 298us/step - loss: 0.0464 - acc: 0.9829 - val_loss: 0.5573 - val_acc: 0.8785\n",
      "Epoch 19/30\n",
      "52684/52684 [==============================] - 16s 302us/step - loss: 0.0426 - acc: 0.9850 - val_loss: 0.5403 - val_acc: 0.8794\n",
      "Epoch 20/30\n",
      "52684/52684 [==============================] - 16s 299us/step - loss: 0.0394 - acc: 0.9851 - val_loss: 0.5420 - val_acc: 0.8752\n",
      "Epoch 21/30\n",
      "52684/52684 [==============================] - 18s 338us/step - loss: 0.0406 - acc: 0.9854 - val_loss: 0.5023 - val_acc: 0.8774\n",
      "Epoch 22/30\n",
      "52684/52684 [==============================] - 16s 306us/step - loss: 0.0370 - acc: 0.9860 - val_loss: 0.6290 - val_acc: 0.8788\n",
      "Epoch 23/30\n",
      "52684/52684 [==============================] - 17s 315us/step - loss: 0.0375 - acc: 0.9868 - val_loss: 0.5963 - val_acc: 0.8806\n",
      "Epoch 24/30\n",
      "52684/52684 [==============================] - 16s 300us/step - loss: 0.0336 - acc: 0.9879 - val_loss: 0.6774 - val_acc: 0.8788\n",
      "Epoch 25/30\n",
      "52684/52684 [==============================] - 16s 300us/step - loss: 0.0348 - acc: 0.9873 - val_loss: 0.6235 - val_acc: 0.8799\n",
      "Epoch 26/30\n",
      "52684/52684 [==============================] - 16s 298us/step - loss: 0.0310 - acc: 0.9887 - val_loss: 0.6643 - val_acc: 0.8806\n",
      "Epoch 27/30\n",
      "52684/52684 [==============================] - 16s 305us/step - loss: 0.0317 - acc: 0.9888 - val_loss: 0.6855 - val_acc: 0.8763\n",
      "Epoch 28/30\n",
      "52684/52684 [==============================] - 16s 303us/step - loss: 0.0314 - acc: 0.9887 - val_loss: 0.6029 - val_acc: 0.8785\n",
      "Epoch 29/30\n",
      "52684/52684 [==============================] - 16s 301us/step - loss: 0.0299 - acc: 0.9893 - val_loss: 0.5910 - val_acc: 0.8791\n",
      "Epoch 30/30\n",
      "52684/52684 [==============================] - 16s 301us/step - loss: 0.0278 - acc: 0.9899 - val_loss: 0.6351 - val_acc: 0.8841\n"
     ]
    }
   ],
   "source": [
    "if (True):\n",
    "    X_train = D.data['X_train']\n",
    "    Y_train = D.data['Y_train']\n",
    "    M.fit(X_train, Y_train)                     \n",
    "\n",
    "Y_hat_train = M.predict(D.data['X_train'])\n",
    "Y_hat_valid = M.predict(D.data['X_valid'])\n",
    "Y_hat_test = M.predict(D.data['X_test'])\n",
    "if(os.path.exists(\"public_data/perso_test.solution\") and os.path.exists(\"public_data/perso_valid.solution\")) : \n",
    "    Y_test = D.data['Y_test']\n",
    "    Y_valid = D.data['Y_valid']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <b> Save the trained model </b> (will be ready to reload next time around) and save the prediction results. IMPORTANT: if you save the trained model, it will be bundled with your sample code submission. Therefore your model will NOT be retrained on the challenge platform. Remove the pickle from the submission if you want the model to be retrained on the platform.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 506,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'ls' n’est pas reconnu en tant que commande interne\n",
      "ou externe, un programme exécutable ou un fichier de commandes.\n"
     ]
    }
   ],
   "source": [
    "M.save(trained_model_name)                 \n",
    "result_name = result_dir + data_name\n",
    "from data_io import write\n",
    "write(result_name + '_train.predict', Y_hat_train)\n",
    "write(result_name + '_valid.predict', Y_hat_valid)\n",
    "write(result_name + '_test.predict', Y_hat_test)\n",
    "!ls $result_name*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 507,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "65856\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_hat_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h2>Scoring the results</h2>\n",
    "    <h3>Load the challenge metric</h3>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Metric : AUC_metric \n",
    "ROC curves are typically used in binary classification to study the output of a classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using scoring metric: roc_auc_score\n"
     ]
    }
   ],
   "source": [
    "from libscores import get_metric\n",
    "metric_name, scoring_function = get_metric()\n",
    "print('Using scoring metric:', metric_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h3> Training performance </h3>\n",
    "    <p>\n",
    "The participants normally posess target values (labels) only for training examples (except for the sample data). We compute with the `example` metric the training score, which should be zero for perfect predictions.\n",
    "        </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 509,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "n_classes=2\n",
    "def fpr_tpr(solution, prediction):\n",
    "    for i in range(n_classes):\n",
    "        fpr, tpr, _ = metrics.roc_curve(solution, prediction)\n",
    "        roc_auc = metrics.auc(fpr, tpr)\n",
    "    return (fpr,tpr)\n",
    "def p2c(prediction,threshold=0.5) : \n",
    "    c = []\n",
    "    for ele in prediction : \n",
    "        if(ele>=0.5) : \n",
    "            c.append(1)\n",
    "        else : \n",
    "            c.append(0)\n",
    "    return np.array(c)\n",
    "\n",
    "def plot_cm_matrix(solution,prediction,title) :\n",
    "    prediction = p2c(prediction)\n",
    "    cm = confusion_matrix(solution, prediction)\n",
    "    df_cm = pd.DataFrame(cm, index = [i for i in \"01\"],columns = [i for i in \"01\"])\n",
    "    plt.figure(figsize = (5,3))\n",
    "    sn.heatmap(df_cm, annot=True)\n",
    "    plt.title(title)\n",
    "\n",
    "def plot_ROC(fpr,tpr,title) :\n",
    "    plt.figure()\n",
    "    lw = 2\n",
    "    plt.plot(fpr, tpr, color='darkorange',lw=lw)\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "if(False):##os.path.exists(\"sample_data/perso_test.solution\") and os.path.exists(\"sample_data/perso_valid.solution\")) : \n",
    "    fpr_train,tpr_train = fpr_tpr(Y_train, Y_hat_train)\n",
    "    fpr_test,tpr_test = fpr_tpr(Y_test, Y_hat_test)\n",
    "    fpr_valid,tpr_valid = fpr_tpr(Y_valid, Y_hat_valid)\n",
    "\n",
    "    print('Training score for the', metric_name, 'metric = %5.4f' % scoring_function(Y_train, Y_hat_train))\n",
    "    print('Ideal score for the', metric_name, 'metric = %5.4f' % scoring_function(Y_train, Y_train))\n",
    "    print('Test score for the', metric_name, 'metric = %5.4f' % scoring_function(Y_test, Y_hat_test))\n",
    "    print('Valid score for the', metric_name, 'metric = %5.4f' % scoring_function(Y_valid, Y_hat_valid))\n",
    "\n",
    "    plot_cm_matrix(Y_train,Y_hat_train,\"Confusion matrix for train data\") \n",
    "    plot_ROC(fpr_train,tpr_train,\"ROC curve for train data\")\n",
    "    plot_cm_matrix(Y_test,Y_hat_test,\"Confusion matrix for test data\") \n",
    "    plot_ROC(fpr_test,tpr_test,\"ROC curve for test data\")\n",
    "    plot_cm_matrix(Y_valid,Y_hat_valid,\"Confusion matrix for valid data\") \n",
    "    plot_ROC(fpr_valid,tpr_valid,\"ROC curve for valid data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h3>Cross-validation performance</h3>\n",
    "    <p>\n",
    "The participants do not have access to the labels Y_valid and Y_test to self-assess their validation and test performances. But training performance is not a good prediction of validation or test performance. Using cross-validation, the training data is split into multiple training/test folds, which allows participants to self-assess their model during development. The average CV result and 95% confidence interval is displayed.\n",
    "   </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 510,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 42147 samples, validate on 10537 samples\n",
      "Epoch 1/30\n",
      "42147/42147 [==============================] - 21s 494us/step - loss: 0.5194 - acc: 0.7431 - val_loss: 0.4493 - val_acc: 0.7968\n",
      "Epoch 2/30\n",
      "42147/42147 [==============================] - 13s 314us/step - loss: 0.3839 - acc: 0.8300 - val_loss: 0.4050 - val_acc: 0.8194\n",
      "Epoch 3/30\n",
      "42147/42147 [==============================] - 13s 315us/step - loss: 0.3044 - acc: 0.8713 - val_loss: 0.3779 - val_acc: 0.8394\n",
      "Epoch 4/30\n",
      "42147/42147 [==============================] - 14s 324us/step - loss: 0.2472 - acc: 0.8979 - val_loss: 0.3743 - val_acc: 0.8455\n",
      "Epoch 5/30\n",
      "42147/42147 [==============================] - 13s 309us/step - loss: 0.2039 - acc: 0.9172 - val_loss: 0.3812 - val_acc: 0.8620\n",
      "Epoch 6/30\n",
      "42147/42147 [==============================] - 13s 313us/step - loss: 0.1727 - acc: 0.9298 - val_loss: 0.3574 - val_acc: 0.8576\n",
      "Epoch 7/30\n",
      "42147/42147 [==============================] - 13s 308us/step - loss: 0.1468 - acc: 0.9411 - val_loss: 0.3743 - val_acc: 0.8645\n",
      "Epoch 8/30\n",
      "42147/42147 [==============================] - 13s 314us/step - loss: 0.1264 - acc: 0.9506 - val_loss: 0.4022 - val_acc: 0.8566\n",
      "Epoch 9/30\n",
      "42147/42147 [==============================] - 14s 325us/step - loss: 0.1095 - acc: 0.9569 - val_loss: 0.4536 - val_acc: 0.8619\n",
      "Epoch 10/30\n",
      "42147/42147 [==============================] - 13s 314us/step - loss: 0.0963 - acc: 0.9633 - val_loss: 0.4696 - val_acc: 0.8625\n",
      "Epoch 11/30\n",
      "42147/42147 [==============================] - 14s 320us/step - loss: 0.0860 - acc: 0.9671 - val_loss: 0.6144 - val_acc: 0.8588\n",
      "Epoch 12/30\n",
      "42147/42147 [==============================] - 13s 318us/step - loss: 0.0773 - acc: 0.9709 - val_loss: 0.4683 - val_acc: 0.8650\n",
      "Epoch 13/30\n",
      "42147/42147 [==============================] - 13s 318us/step - loss: 0.0680 - acc: 0.9741 - val_loss: 0.5648 - val_acc: 0.8619\n",
      "Epoch 14/30\n",
      "42147/42147 [==============================] - 13s 318us/step - loss: 0.0638 - acc: 0.9751 - val_loss: 0.6240 - val_acc: 0.8623\n",
      "Epoch 15/30\n",
      "42147/42147 [==============================] - 14s 321us/step - loss: 0.0596 - acc: 0.9778 - val_loss: 0.4975 - val_acc: 0.8589\n",
      "Epoch 16/30\n",
      "42147/42147 [==============================] - 13s 317us/step - loss: 0.0547 - acc: 0.9793 - val_loss: 0.5274 - val_acc: 0.8601\n",
      "Epoch 17/30\n",
      "42147/42147 [==============================] - 13s 320us/step - loss: 0.0519 - acc: 0.9804 - val_loss: 0.5392 - val_acc: 0.8647\n",
      "Epoch 18/30\n",
      "42147/42147 [==============================] - 13s 320us/step - loss: 0.0495 - acc: 0.9819 - val_loss: 0.6148 - val_acc: 0.8557\n",
      "Epoch 19/30\n",
      "42147/42147 [==============================] - 14s 322us/step - loss: 0.0446 - acc: 0.9838 - val_loss: 0.6719 - val_acc: 0.8648\n",
      "Epoch 20/30\n",
      "42147/42147 [==============================] - 14s 321us/step - loss: 0.0414 - acc: 0.9849 - val_loss: 0.6055 - val_acc: 0.8619\n",
      "Epoch 21/30\n",
      "42147/42147 [==============================] - 14s 321us/step - loss: 0.0431 - acc: 0.9848 - val_loss: 0.5783 - val_acc: 0.8663\n",
      "Epoch 22/30\n",
      "42147/42147 [==============================] - 14s 324us/step - loss: 0.0401 - acc: 0.9862 - val_loss: 0.5841 - val_acc: 0.8693\n",
      "Epoch 23/30\n",
      "42147/42147 [==============================] - 13s 319us/step - loss: 0.0416 - acc: 0.9856 - val_loss: 0.6411 - val_acc: 0.8667\n",
      "Epoch 24/30\n",
      "42147/42147 [==============================] - 14s 325us/step - loss: 0.0456 - acc: 0.9839 - val_loss: 0.6746 - val_acc: 0.8641\n",
      "Epoch 25/30\n",
      "42147/42147 [==============================] - 14s 323us/step - loss: 0.0364 - acc: 0.9867 - val_loss: 0.8599 - val_acc: 0.8488\n",
      "Epoch 26/30\n",
      "42147/42147 [==============================] - 14s 321us/step - loss: 0.0376 - acc: 0.9867 - val_loss: 0.6361 - val_acc: 0.8671\n",
      "Epoch 27/30\n",
      "42147/42147 [==============================] - 14s 335us/step - loss: 0.0352 - acc: 0.9880 - val_loss: 0.4909 - val_acc: 0.8599\n",
      "Epoch 28/30\n",
      "42147/42147 [==============================] - 14s 323us/step - loss: 0.0337 - acc: 0.9884 - val_loss: 0.7186 - val_acc: 0.8641\n",
      "Epoch 29/30\n",
      "42147/42147 [==============================] - 14s 321us/step - loss: 0.0306 - acc: 0.9891 - val_loss: 0.6991 - val_acc: 0.8684\n",
      "Epoch 30/30\n",
      "42147/42147 [==============================] - 14s 321us/step - loss: 0.0299 - acc: 0.9898 - val_loss: 0.6779 - val_acc: 0.8696\n",
      "Train on 42148 samples, validate on 10537 samples\n",
      "Epoch 1/30\n",
      "42148/42148 [==============================] - 21s 497us/step - loss: 0.5230 - acc: 0.7407 - val_loss: 0.4531 - val_acc: 0.7939\n",
      "Epoch 2/30\n",
      "42148/42148 [==============================] - 13s 314us/step - loss: 0.3926 - acc: 0.8276 - val_loss: 0.3933 - val_acc: 0.8243\n",
      "Epoch 3/30\n",
      "42148/42148 [==============================] - 13s 314us/step - loss: 0.3076 - acc: 0.8697 - val_loss: 0.3633 - val_acc: 0.8497\n",
      "Epoch 4/30\n",
      "42148/42148 [==============================] - 13s 310us/step - loss: 0.2496 - acc: 0.8970 - val_loss: 0.3743 - val_acc: 0.8493\n",
      "Epoch 5/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.2064 - acc: 0.9144 - val_loss: 0.3706 - val_acc: 0.8535\n",
      "Epoch 6/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.1724 - acc: 0.9311 - val_loss: 0.3720 - val_acc: 0.8632\n",
      "Epoch 7/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.1435 - acc: 0.9438 - val_loss: 0.3884 - val_acc: 0.8619\n",
      "Epoch 8/30\n",
      "42148/42148 [==============================] - 13s 313us/step - loss: 0.1222 - acc: 0.9534 - val_loss: 0.3837 - val_acc: 0.8637\n",
      "Epoch 9/30\n",
      "42148/42148 [==============================] - 13s 318us/step - loss: 0.1055 - acc: 0.9587 - val_loss: 0.4671 - val_acc: 0.8616\n",
      "Epoch 10/30\n",
      "42148/42148 [==============================] - 13s 310us/step - loss: 0.0925 - acc: 0.9634 - val_loss: 0.4503 - val_acc: 0.8633\n",
      "Epoch 11/30\n",
      "42148/42148 [==============================] - 13s 315us/step - loss: 0.0786 - acc: 0.9701 - val_loss: 0.4604 - val_acc: 0.8577\n",
      "Epoch 12/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.0729 - acc: 0.9722 - val_loss: 0.5244 - val_acc: 0.8641\n",
      "Epoch 13/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.0648 - acc: 0.9753 - val_loss: 0.5823 - val_acc: 0.8536\n",
      "Epoch 14/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.0608 - acc: 0.9776 - val_loss: 0.5263 - val_acc: 0.8563\n",
      "Epoch 15/30\n",
      "42148/42148 [==============================] - 13s 309us/step - loss: 0.0554 - acc: 0.9803 - val_loss: 0.5674 - val_acc: 0.8571\n",
      "Epoch 16/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.0554 - acc: 0.9788 - val_loss: 0.5409 - val_acc: 0.8640\n",
      "Epoch 17/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.0494 - acc: 0.9822 - val_loss: 0.5518 - val_acc: 0.8666\n",
      "Epoch 18/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.0450 - acc: 0.9838 - val_loss: 0.5842 - val_acc: 0.8640\n",
      "Epoch 19/30\n",
      "42148/42148 [==============================] - 13s 313us/step - loss: 0.0460 - acc: 0.9833 - val_loss: 0.5830 - val_acc: 0.8643\n",
      "Epoch 20/30\n",
      "42148/42148 [==============================] - 13s 317us/step - loss: 0.0423 - acc: 0.9849 - val_loss: 0.6433 - val_acc: 0.8652\n",
      "Epoch 21/30\n",
      "42148/42148 [==============================] - 13s 309us/step - loss: 0.0390 - acc: 0.9864 - val_loss: 0.6053 - val_acc: 0.8639\n",
      "Epoch 22/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.0385 - acc: 0.9860 - val_loss: 0.5439 - val_acc: 0.8660\n",
      "Epoch 23/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.0389 - acc: 0.9858 - val_loss: 0.6083 - val_acc: 0.8687\n",
      "Epoch 24/30\n",
      "42148/42148 [==============================] - 13s 313us/step - loss: 0.0326 - acc: 0.9882 - val_loss: 0.5976 - val_acc: 0.8669\n",
      "Epoch 25/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.0324 - acc: 0.9884 - val_loss: 0.7069 - val_acc: 0.8613\n",
      "Epoch 26/30\n",
      "42148/42148 [==============================] - 13s 309us/step - loss: 0.0313 - acc: 0.9888 - val_loss: 0.6520 - val_acc: 0.8658\n",
      "Epoch 27/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.0284 - acc: 0.9897 - val_loss: 0.7654 - val_acc: 0.8682\n",
      "Epoch 28/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.0274 - acc: 0.9902 - val_loss: 0.8068 - val_acc: 0.8672\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30\n",
      "42148/42148 [==============================] - 13s 301us/step - loss: 0.0295 - acc: 0.9894 - val_loss: 0.5881 - val_acc: 0.8630\n",
      "Epoch 30/30\n",
      "42148/42148 [==============================] - 13s 299us/step - loss: 0.0263 - acc: 0.9902 - val_loss: 0.8328 - val_acc: 0.8705\n",
      "Train on 42148 samples, validate on 10537 samples\n",
      "Epoch 1/30\n",
      "42148/42148 [==============================] - 20s 486us/step - loss: 0.5230 - acc: 0.7368 - val_loss: 0.4559 - val_acc: 0.7865\n",
      "Epoch 2/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.3899 - acc: 0.8264 - val_loss: 0.4023 - val_acc: 0.8230\n",
      "Epoch 3/30\n",
      "42148/42148 [==============================] - 13s 310us/step - loss: 0.3090 - acc: 0.8685 - val_loss: 0.3633 - val_acc: 0.8435\n",
      "Epoch 4/30\n",
      "42148/42148 [==============================] - 13s 309us/step - loss: 0.2516 - acc: 0.8967 - val_loss: 0.3607 - val_acc: 0.8507\n",
      "Epoch 5/30\n",
      "42148/42148 [==============================] - 13s 309us/step - loss: 0.2109 - acc: 0.9141 - val_loss: 0.3570 - val_acc: 0.8540\n",
      "Epoch 6/30\n",
      "42148/42148 [==============================] - 13s 308us/step - loss: 0.1780 - acc: 0.9297 - val_loss: 0.3635 - val_acc: 0.8627\n",
      "Epoch 7/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.1528 - acc: 0.9383 - val_loss: 0.4404 - val_acc: 0.8633\n",
      "Epoch 8/30\n",
      "42148/42148 [==============================] - 13s 310us/step - loss: 0.1274 - acc: 0.9504 - val_loss: 0.4015 - val_acc: 0.8534\n",
      "Epoch 9/30\n",
      "42148/42148 [==============================] - 13s 310us/step - loss: 0.1102 - acc: 0.9565 - val_loss: 0.4248 - val_acc: 0.8640\n",
      "Epoch 10/30\n",
      "42148/42148 [==============================] - 13s 310us/step - loss: 0.0988 - acc: 0.9626 - val_loss: 0.4324 - val_acc: 0.8635\n",
      "Epoch 11/30\n",
      "42148/42148 [==============================] - 13s 307us/step - loss: 0.0877 - acc: 0.9660 - val_loss: 0.4806 - val_acc: 0.8591\n",
      "Epoch 12/30\n",
      "42148/42148 [==============================] - 13s 309us/step - loss: 0.0814 - acc: 0.9692 - val_loss: 0.4985 - val_acc: 0.8645\n",
      "Epoch 13/30\n",
      "42148/42148 [==============================] - 13s 310us/step - loss: 0.0695 - acc: 0.9721 - val_loss: 0.6122 - val_acc: 0.8641\n",
      "Epoch 14/30\n",
      "42148/42148 [==============================] - 13s 313us/step - loss: 0.0623 - acc: 0.9765 - val_loss: 0.6112 - val_acc: 0.8631\n",
      "Epoch 15/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.0613 - acc: 0.9770 - val_loss: 0.5330 - val_acc: 0.8659\n",
      "Epoch 16/30\n",
      "42148/42148 [==============================] - 13s 310us/step - loss: 0.0547 - acc: 0.9798 - val_loss: 0.6093 - val_acc: 0.8613\n",
      "Epoch 17/30\n",
      "42148/42148 [==============================] - 13s 313us/step - loss: 0.0514 - acc: 0.9810 - val_loss: 0.5859 - val_acc: 0.8638\n",
      "Epoch 18/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.0489 - acc: 0.9821 - val_loss: 0.6015 - val_acc: 0.8623\n",
      "Epoch 19/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.0446 - acc: 0.9837 - val_loss: 0.6680 - val_acc: 0.8665\n",
      "Epoch 20/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.0455 - acc: 0.9829 - val_loss: 0.6053 - val_acc: 0.8682\n",
      "Epoch 21/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.0435 - acc: 0.9851 - val_loss: 0.6039 - val_acc: 0.8690\n",
      "Epoch 22/30\n",
      "42148/42148 [==============================] - 13s 309us/step - loss: 0.0456 - acc: 0.9836 - val_loss: 0.6965 - val_acc: 0.8669\n",
      "Epoch 23/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.0385 - acc: 0.9870 - val_loss: 0.5474 - val_acc: 0.8674\n",
      "Epoch 24/30\n",
      "42148/42148 [==============================] - 13s 313us/step - loss: 0.0375 - acc: 0.9870 - val_loss: 0.6333 - val_acc: 0.8670\n",
      "Epoch 25/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.0408 - acc: 0.9849 - val_loss: 0.6433 - val_acc: 0.8720\n",
      "Epoch 26/30\n",
      "42148/42148 [==============================] - 13s 314us/step - loss: 0.0306 - acc: 0.9886 - val_loss: 0.6306 - val_acc: 0.8695\n",
      "Epoch 27/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.0320 - acc: 0.9890 - val_loss: 0.6607 - val_acc: 0.8612\n",
      "Epoch 28/30\n",
      "42148/42148 [==============================] - 13s 310us/step - loss: 0.0303 - acc: 0.9890 - val_loss: 0.7238 - val_acc: 0.8705\n",
      "Epoch 29/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.0299 - acc: 0.9898 - val_loss: 0.6882 - val_acc: 0.8644\n",
      "Epoch 30/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.0299 - acc: 0.9894 - val_loss: 0.6467 - val_acc: 0.8700\n",
      "Train on 42148 samples, validate on 10537 samples\n",
      "Epoch 1/30\n",
      "42148/42148 [==============================] - 21s 506us/step - loss: 0.5221 - acc: 0.7412 - val_loss: 0.4745 - val_acc: 0.7806\n",
      "Epoch 2/30\n",
      "42148/42148 [==============================] - 13s 312us/step - loss: 0.3909 - acc: 0.8276 - val_loss: 0.3876 - val_acc: 0.8302\n",
      "Epoch 3/30\n",
      "42148/42148 [==============================] - 13s 314us/step - loss: 0.3039 - acc: 0.8709 - val_loss: 0.3504 - val_acc: 0.8489\n",
      "Epoch 4/30\n",
      "42148/42148 [==============================] - 13s 316us/step - loss: 0.2448 - acc: 0.8982 - val_loss: 0.3668 - val_acc: 0.8472\n",
      "Epoch 5/30\n",
      "42148/42148 [==============================] - 13s 315us/step - loss: 0.2056 - acc: 0.9163 - val_loss: 0.3549 - val_acc: 0.8514\n",
      "Epoch 6/30\n",
      "42148/42148 [==============================] - 13s 316us/step - loss: 0.1725 - acc: 0.9301 - val_loss: 0.3685 - val_acc: 0.8626\n",
      "Epoch 7/30\n",
      "42148/42148 [==============================] - 13s 317us/step - loss: 0.1421 - acc: 0.9435 - val_loss: 0.3464 - val_acc: 0.8695\n",
      "Epoch 8/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.1257 - acc: 0.9499 - val_loss: 0.3887 - val_acc: 0.8611\n",
      "Epoch 9/30\n",
      "42148/42148 [==============================] - 13s 315us/step - loss: 0.1077 - acc: 0.9572 - val_loss: 0.4871 - val_acc: 0.8677\n",
      "Epoch 10/30\n",
      "42148/42148 [==============================] - 13s 315us/step - loss: 0.0963 - acc: 0.9627 - val_loss: 0.4661 - val_acc: 0.8648\n",
      "Epoch 11/30\n",
      "42148/42148 [==============================] - 13s 315us/step - loss: 0.0863 - acc: 0.9670 - val_loss: 0.5127 - val_acc: 0.8613\n",
      "Epoch 12/30\n",
      "42148/42148 [==============================] - 13s 314us/step - loss: 0.0757 - acc: 0.9715 - val_loss: 0.4861 - val_acc: 0.8659\n",
      "Epoch 13/30\n",
      "42148/42148 [==============================] - 15s 365us/step - loss: 0.0689 - acc: 0.9742 - val_loss: 0.5334 - val_acc: 0.8609\n",
      "Epoch 14/30\n",
      "42148/42148 [==============================] - 14s 326us/step - loss: 0.0633 - acc: 0.9762 - val_loss: 0.5633 - val_acc: 0.8674\n",
      "Epoch 15/30\n",
      "42148/42148 [==============================] - 14s 341us/step - loss: 0.0589 - acc: 0.9785 - val_loss: 0.4738 - val_acc: 0.8642\n",
      "Epoch 16/30\n",
      "42148/42148 [==============================] - 14s 337us/step - loss: 0.0542 - acc: 0.9797 - val_loss: 0.6026 - val_acc: 0.8689\n",
      "Epoch 17/30\n",
      "42148/42148 [==============================] - 13s 313us/step - loss: 0.0484 - acc: 0.9818 - val_loss: 0.5961 - val_acc: 0.8694\n",
      "Epoch 18/30\n",
      "42148/42148 [==============================] - 14s 328us/step - loss: 0.0471 - acc: 0.9823 - val_loss: 0.5889 - val_acc: 0.8672\n",
      "Epoch 19/30\n",
      "42148/42148 [==============================] - 13s 314us/step - loss: 0.0437 - acc: 0.9837 - val_loss: 0.6358 - val_acc: 0.8675\n",
      "Epoch 20/30\n",
      "42148/42148 [==============================] - 13s 314us/step - loss: 0.0410 - acc: 0.9848 - val_loss: 0.6106 - val_acc: 0.8705\n",
      "Epoch 21/30\n",
      "42148/42148 [==============================] - 14s 321us/step - loss: 0.0415 - acc: 0.9849 - val_loss: 0.6053 - val_acc: 0.8694\n",
      "Epoch 22/30\n",
      "42148/42148 [==============================] - 13s 318us/step - loss: 0.0391 - acc: 0.9862 - val_loss: 0.5678 - val_acc: 0.8682\n",
      "Epoch 23/30\n",
      "42148/42148 [==============================] - 13s 313us/step - loss: 0.0377 - acc: 0.9864 - val_loss: 0.6598 - val_acc: 0.8687\n",
      "Epoch 24/30\n",
      "42148/42148 [==============================] - 13s 314us/step - loss: 0.0339 - acc: 0.9880 - val_loss: 0.6145 - val_acc: 0.8746\n",
      "Epoch 25/30\n",
      "42148/42148 [==============================] - 13s 313us/step - loss: 0.0523 - acc: 0.9811 - val_loss: 0.6407 - val_acc: 0.8705\n",
      "Epoch 26/30\n",
      "42148/42148 [==============================] - 13s 313us/step - loss: 0.0313 - acc: 0.9895 - val_loss: 0.6480 - val_acc: 0.8732\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30\n",
      "42148/42148 [==============================] - 13s 308us/step - loss: 0.0311 - acc: 0.9886 - val_loss: 0.6596 - val_acc: 0.8660\n",
      "Epoch 28/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.0280 - acc: 0.9900 - val_loss: 0.6536 - val_acc: 0.8709\n",
      "Epoch 29/30\n",
      "42148/42148 [==============================] - 13s 311us/step - loss: 0.0307 - acc: 0.9889 - val_loss: 0.7332 - val_acc: 0.8694\n",
      "Epoch 30/30\n",
      "42148/42148 [==============================] - 13s 310us/step - loss: 0.0284 - acc: 0.9906 - val_loss: 0.6519 - val_acc: 0.8693\n",
      "Train on 42148 samples, validate on 10537 samples\n",
      "Epoch 1/30\n",
      "42148/42148 [==============================] - 21s 509us/step - loss: 0.5201 - acc: 0.7410 - val_loss: 0.4499 - val_acc: 0.7942\n",
      "Epoch 2/30\n",
      "42148/42148 [==============================] - 14s 335us/step - loss: 0.3855 - acc: 0.8304 - val_loss: 0.3728 - val_acc: 0.8345\n",
      "Epoch 3/30\n",
      "42148/42148 [==============================] - 16s 374us/step - loss: 0.3014 - acc: 0.8713 - val_loss: 0.3896 - val_acc: 0.8328\n",
      "Epoch 4/30\n",
      "42148/42148 [==============================] - 15s 361us/step - loss: 0.2426 - acc: 0.8993 - val_loss: 0.3585 - val_acc: 0.8535\n",
      "Epoch 5/30\n",
      "42148/42148 [==============================] - 16s 381us/step - loss: 0.1985 - acc: 0.9190 - val_loss: 0.3966 - val_acc: 0.8600\n",
      "Epoch 6/30\n",
      "42148/42148 [==============================] - 14s 337us/step - loss: 0.1659 - acc: 0.9339 - val_loss: 0.3415 - val_acc: 0.8657\n",
      "Epoch 7/30\n",
      "42148/42148 [==============================] - 16s 377us/step - loss: 0.1376 - acc: 0.9454 - val_loss: 0.3613 - val_acc: 0.8693\n",
      "Epoch 8/30\n",
      "42148/42148 [==============================] - 14s 323us/step - loss: 0.1199 - acc: 0.9529 - val_loss: 0.3818 - val_acc: 0.8654\n",
      "Epoch 9/30\n",
      "42148/42148 [==============================] - 14s 333us/step - loss: 0.1043 - acc: 0.9587 - val_loss: 0.4082 - val_acc: 0.8673\n",
      "Epoch 10/30\n",
      "42148/42148 [==============================] - 15s 344us/step - loss: 0.0895 - acc: 0.9657 - val_loss: 0.4641 - val_acc: 0.8680\n",
      "Epoch 11/30\n",
      "42148/42148 [==============================] - 19s 447us/step - loss: 0.0811 - acc: 0.9688 - val_loss: 0.4496 - val_acc: 0.8713\n",
      "Epoch 12/30\n",
      "42148/42148 [==============================] - 15s 367us/step - loss: 0.0723 - acc: 0.9733 - val_loss: 0.4629 - val_acc: 0.8659\n",
      "Epoch 13/30\n",
      "42148/42148 [==============================] - 14s 335us/step - loss: 0.0672 - acc: 0.9747 - val_loss: 0.4730 - val_acc: 0.8725\n",
      "Epoch 14/30\n",
      "42148/42148 [==============================] - 13s 318us/step - loss: 0.0596 - acc: 0.9776 - val_loss: 0.5516 - val_acc: 0.8594\n",
      "Epoch 15/30\n",
      "42148/42148 [==============================] - 14s 324us/step - loss: 0.0565 - acc: 0.9795 - val_loss: 0.4647 - val_acc: 0.8714\n",
      "Epoch 16/30\n",
      "42148/42148 [==============================] - 14s 330us/step - loss: 0.0517 - acc: 0.9809 - val_loss: 0.5460 - val_acc: 0.8691\n",
      "Epoch 17/30\n",
      "42148/42148 [==============================] - 18s 424us/step - loss: 0.0499 - acc: 0.9815 - val_loss: 0.5174 - val_acc: 0.8704\n",
      "Epoch 18/30\n",
      "42148/42148 [==============================] - 16s 387us/step - loss: 0.0453 - acc: 0.9834 - val_loss: 0.5998 - val_acc: 0.8668\n",
      "Epoch 19/30\n",
      "42148/42148 [==============================] - 16s 385us/step - loss: 0.0466 - acc: 0.9825 - val_loss: 0.5696 - val_acc: 0.8688\n",
      "Epoch 20/30\n",
      "42148/42148 [==============================] - 16s 371us/step - loss: 0.0442 - acc: 0.9833 - val_loss: 0.5569 - val_acc: 0.8658\n",
      "Epoch 21/30\n",
      "42148/42148 [==============================] - 15s 361us/step - loss: 0.0438 - acc: 0.9844 - val_loss: 0.5855 - val_acc: 0.8705\n",
      "Epoch 22/30\n",
      "42148/42148 [==============================] - 16s 374us/step - loss: 0.0390 - acc: 0.9857 - val_loss: 0.4975 - val_acc: 0.8672\n",
      "Epoch 23/30\n",
      "42148/42148 [==============================] - 16s 377us/step - loss: 0.0347 - acc: 0.9878 - val_loss: 0.5746 - val_acc: 0.8694\n",
      "Epoch 24/30\n",
      "42148/42148 [==============================] - 17s 407us/step - loss: 0.0327 - acc: 0.9883 - val_loss: 0.4969 - val_acc: 0.8682\n",
      "Epoch 25/30\n",
      "42148/42148 [==============================] - 15s 349us/step - loss: 0.0330 - acc: 0.9885 - val_loss: 0.6432 - val_acc: 0.8710\n",
      "Epoch 26/30\n",
      "42148/42148 [==============================] - 18s 424us/step - loss: 0.0298 - acc: 0.9895 - val_loss: 0.6553 - val_acc: 0.8687\n",
      "Epoch 27/30\n",
      "42148/42148 [==============================] - 15s 367us/step - loss: 0.0295 - acc: 0.9897 - val_loss: 0.6533 - val_acc: 0.8686\n",
      "Epoch 28/30\n",
      "42148/42148 [==============================] - 16s 373us/step - loss: 0.0292 - acc: 0.9895 - val_loss: 0.7694 - val_acc: 0.8658\n",
      "Epoch 29/30\n",
      "42148/42148 [==============================] - 15s 361us/step - loss: 0.0295 - acc: 0.9893 - val_loss: 0.6466 - val_acc: 0.8712\n",
      "Epoch 30/30\n",
      "42148/42148 [==============================] - 18s 421us/step - loss: 0.0329 - acc: 0.9886 - val_loss: 0.6575 - val_acc: 0.8750\n",
      "\n",
      "CV score (95 perc. CI): 0.87 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(M, X_train, Y_train, cv=5, scoring=make_scorer(scoring_function))\n",
    "print('\\nCV score (95 perc. CI): %0.2f (+/- %0.2f)' % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "<h1> Step 3: Making a submission </h1> \n",
    "\n",
    "<h2> Unit testing </h2> \n",
    "\n",
    "It is <b><span style=\"color:red\">important that you test your submission files before submitting them</span></b>. All you have to do to make a submission is modify the file <code>model.py</code> in the <code>sample_code_submission/</code> directory, then run this test to make sure everything works fine. This is the actual program that will be run on the server to test your submission. \n",
    "<br>\n",
    "Keep the sample code simple.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using input_dir: C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\public_data\n",
      "Using output_dir: C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\sample_result_submission\n",
      "Using program_dir: C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\ingestion_program\n",
      "Using submission_dir: C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\sample_code_submission\n",
      "\n",
      "========== Ingestion program version 6 ==========\n",
      "\n",
      "************************************************\n",
      "******** Processing dataset Perso ********\n",
      "************************************************\n",
      "========= Reading and converting data ==========\n",
      "Info file found : C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\public_data\\perso_public.info\n",
      "========= Reading C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\public_data\\perso_feat.type\n",
      "[+] Success in  0.00 sec\n",
      "========= Reading C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\public_data\\perso_train.data\n",
      "Replace missing values by 0 (slow, sorry)\n",
      "[+] Success in  6.14 sec\n",
      "========= Reading C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\public_data\\perso_train.solution\n",
      "[+] Success in  0.17 sec\n",
      "========= Reading C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\public_data\\perso_valid.data\n",
      "Replace missing values by 0 (slow, sorry)\n",
      "[+] Success in  0.80 sec\n",
      "========= Reading C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\public_data\\perso_valid.solution\n",
      "[+] Success in  0.00 sec\n",
      "========= Reading C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\public_data\\perso_test.data\n",
      "Replace missing values by 0 (slow, sorry)\n",
      "[+] Success in  1.55 sec\n",
      "========= Reading C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\public_data\\perso_test.solution\n",
      "[+] Success in  0.00 sec\n",
      "DataManager : perso\n",
      "info:\n",
      "\tusage = Sample dataset perso data\n",
      "\tname = perso\n",
      "\ttask = bi-class.classification\n",
      "\ttarget_type = Numerical\n",
      "\tfeat_type = Numerical\n",
      "\tmetric = bac_metric\n",
      "\ttime_budget = 1200\n",
      "\tfeat_num = 200\n",
      "\ttarget_num = 2\n",
      "\tlabel_num = 2\n",
      "\ttrain_num = 65856\n",
      "\tvalid_num = 9408\n",
      "\ttest_num = 18817\n",
      "\thas_categorical = 0\n",
      "\thas_missing = 0\n",
      "\tis_sparse = 0\n",
      "\tformat = dense\n",
      "data:\n",
      "\tX_train = array(65856, 200)\n",
      "\tY_train = array(65856,)\n",
      "\tX_valid = array(9408, 200)\n",
      "\tY_valid = array(0,)\n",
      "\tX_test = array(18817, 200)\n",
      "\tY_test = array(0,)\n",
      "feat_type:\tarray(200,)\n",
      "feat_idx:\tarray(200,)\n",
      "\n",
      "[+] Size of uploaded data  56.00 bytes\n",
      "[+] Cumulated time budget (all tasks so far)  1200.00 sec\n",
      "[+] Time budget for this task 1200.00 sec\n",
      "[+] Remaining time after reading data 1191.10 sec\n",
      "======== Creating model ==========\n",
      "**********************************************************\n",
      "****** Attempting to reload model to avoid training ******\n",
      "**********************************************************\n",
      "Model reloaded from: C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\sample_code_submission\\perso_model.pickle\n",
      "[+] Model reloaded, no need to train!\n",
      "[+] Prediction success, time spent so far 11.72 sec\n",
      "======== Saving results to: C:\\Users\\mlans\\Documents\\L2\\Mini-Projet\\Prepross\\starting_kit_c1_final\\sample_result_submission\n",
      "[+] Results saved, time spent so far 11.98 sec\n",
      "[+] End cycle, time left 1188.02 sec\n",
      "[+] Done\n",
      "[+] Overall time spent 16.62 sec ::  Overall time budget 1200.00 sec\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "WARNING:tensorflow:From C:\\Users\\mlans\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "2019-04-07 17:37:45.524357: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2\n",
      "WARNING:tensorflow:From C:\\Users\\mlans\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n"
     ]
    }
   ],
   "source": [
    "!python $problem_dir/ingestion.py $data_dir $result_dir $problem_dir $model_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "Also test the scoring program:\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======= Set 1 (Perso_train): roc_auc_score(set1_score)=0.992245691502 =======\n"
     ]
    }
   ],
   "source": [
    "scoring_output_dir = '../scoring_output_dir'\n",
    "!python $score_dir/score.py $data_dir $result_dir $scoring_output_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<div style=\"background:#FFFFAA\">\n",
    "    <h1> Preparing the submission </h1>\n",
    "\n",
    "Zip the contents of `sample_code_submission/` (without the directory), or download the challenge public_data and run the command in the previous cell, after replacing sample_data by public_data.\n",
    "Then zip the contents of `sample_result_submission/` (without the directory).\n",
    "<b><span style=\"color:red\">Do NOT zip the data with your submissions</span></b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submit one of these files:\n",
      "sample_code_submission_19-04-07-17-37.zip\n",
      "sample_result_submission_19-04-07-17-37.zip\n"
     ]
    }
   ],
   "source": [
    "import datetime \n",
    "from data_io import zipdir\n",
    "the_date = datetime.datetime.now().strftime(\"%y-%m-%d-%H-%M\")\n",
    "sample_code_submission = 'sample_code_submission_' + the_date + '.zip'\n",
    "sample_result_submission = 'sample_result_submission_' + the_date + '.zip'\n",
    "zipdir(sample_code_submission, model_dir)\n",
    "zipdir(sample_result_submission, result_dir)\n",
    "print(\"Submit one of these files:\\n\" + sample_code_submission + \"\\n\" + sample_result_submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_train,tpr_train = fpr_tpr(Y_train, Y_hat_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score for the roc_auc_score metric = 0.9730\n",
      "Ideal score for the roc_auc_score metric = 1.0000\n"
     ]
    }
   ],
   "source": [
    "print('Training score for the', metric_name, 'metric = %5.4f' % scoring_function(Y_train, Y_hat_train))\n",
    "print('Ideal score for the', metric_name, 'metric = %5.4f' % scoring_function(Y_train, Y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_cm_matrix(Y_train,Y_hat_train,\"Confusion matrix for train data\") \n",
    "plot_ROC(fpr_train,tpr_train,\"ROC curve for train data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
